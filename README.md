# ğŸ  Dublin Property Scraper

A modular Python tool for scraping, cleaning, and structuring real estate listing data in Dublin â€” built to explore the housing market and implement modern data engineering practices along the way.

---

## ğŸ§­ Motivation / Background

After moving to Dublin and taking some time between jobs, I found myself naturally curious about the local housing market. Rather than browse listings manually, I turned that curiosity into a hands-on project â€” applying Python, web scraping, and data engineering tools to build a structured, queryable dataset of property listings. What started as a learning exercise grew into a useful tool and a clean, modular project.

---

## âœ¨ Features

- Automated web scraping of paginated listing pages using Selenium
- Extraction of structured listing metadata in JSON format
- Staging `.ndjson` storage using MinIO-compatible object storage
- Cleaning and flattening of nested fields (e.g., address, price, accommodation summary)
- Conversion to columnar `.parquet` files and Delta Lake format
- Adapted medallion-style architecture

---

## ğŸ›  Tech Stack

- **Language**: Python 3.11  
- **Scraping**: Selenium, undetected-chromedriver  
- **Data Processing**: Polars  
- **Storage**: MinIO (S3-compatible), Delta Lake  
- **Package Manager**: [uv](https://github.com/astral-sh/uv) (ultrafast Python dependency manager)  
- **Other Tools**: pyarrow, boto3

---

## ğŸ—‚ Project Structure
```
.
â”œâ”€â”€ pipelines
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ delta_lake.py
â”‚Â Â  â”œâ”€â”€ helper_functions.py
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”œâ”€â”€ min_io_ingestion.py
â”‚Â Â  â”œâ”€â”€ minio_upload.py
â”‚Â Â  â””â”€â”€ scrape_props.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scraped_data
â”‚Â Â  â””â”€â”€ __init__.py
â”œ
â””â”€â”€ test
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test.py

```

## âš™ï¸ Installation & Setup

> Requires Python 3.11+, MinIO server, and `uv` installed.

1. **Clone the repository**
   ```bash
   git clone https://github.com/abhibalu/web2warehouse.git
   cd real_estate_pipeline
2. Install dependencies using uv
uv venv
source .venv/bin/activate
uv pip install -r requirements.lock

Configure MinIO access
Set your AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_ENDPOINT_URL as environment variables.

### ğŸš€ Usage

**Scrape property listing pages:**

```bash
python scraper.py
```
**Convert to .ndjson and upload to MinIO:**
```
python minio_upload.py
```
**Create staging and Cleaned Intermediate Delta Lake table from raw JSON:**
```
python delta_lake.py --layer stg + Intermediate

```

### ğŸ” Output

- Staging Layer: Raw nested .parquet files stored in ./data/stg/
- Silver Layer: Cleaned, flattened .parquet files stored in ./data/silver/

### ğŸ”­ Future Work / Roadmap

- â³ Add orchestration with Dagster (Currently in dev)
- ğŸ§± Load silver layer into a data warehouse (Druid / DuckDB)
- ğŸ“Š Build a dashboard in Apache Superset or Streamlit
- ğŸ§ª Add Pytest unit tests for cleaning + flattening functions



âš ï¸ Disclaimer
This project is for educational and portfolio purposes only. It does not make use of or reference any specific websiteâ€™s API or endorse any particular platform. Please use responsibly.

ğŸ™Œ Acknowledgments
Built with curiosity, coffee, and way too many open browser tabs.


